{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mirtorande/muffin-vs-chihuahua/blob/main/MuffinsChihuahuas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-UYPRw5OWLS"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -qq wandb\n",
        "import wandb\n",
        "from wandb.keras import WandbMetricsLogger\n",
        "wandb.login()\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold, KFold"
      ],
      "metadata": {
        "id": "jX3ONMhsDk7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a config dictionary object\n",
        "configs = dict(\n",
        "    image_size = 64,\n",
        "    #batch_size= 32,\n",
        "    #init_learning_rate = 1e-3,\n",
        "    epochs = 50,\n",
        "    #dropout = 0.1,\n",
        "    optimizer = 'adam',\n",
        "    loss_fn = 'binary_crossentropy',\n",
        "    metrics = ['acc'],\n",
        ")"
      ],
      "metadata": {
        "id": "34tOTS_s_l1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05LIIOopOWLV"
      },
      "source": [
        "# Load the data: the Muffins vs Chihuahuas dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmqOEOyYOWLV"
      },
      "source": [
        "## Retrieve archive\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VyakI2wNWVvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/drive/My Drive/Datasets/archive\""
      ],
      "metadata": {
        "id": "zS4nkIlmXlwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OzI0KLSOWLW"
      },
      "source": [
        "## Filter out corrupted images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIXy-185OWLX"
      },
      "outputs": [],
      "source": [
        "\"\"\"num_skipped = 0\n",
        "for folder_name in ('test/chihuahua', 'train/chihuahua', 'test/muffin', 'train/muffin'):\n",
        "    folder_path = os.path.join(dataset_path, folder_name)\n",
        "    for fname in tqdm(os.listdir(folder_path)):\n",
        "        fpath = os.path.join(folder_path, fname)\n",
        "        try:\n",
        "            fobj = open(fpath, \"rb\")\n",
        "            is_jfif = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10)\n",
        "        finally:\n",
        "            fobj.close()\n",
        "\n",
        "        if not is_jfif:\n",
        "            num_skipped += 1\n",
        "            # Delete corrupted image\n",
        "            os.remove(fpath)\n",
        "\n",
        "print(\"Deleted %d images\" % num_skipped)\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJXQNBbxOWLX"
      },
      "source": [
        "## Generate a dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVT_hk0aOWLY"
      },
      "outputs": [],
      "source": [
        "def load_datasets(config, use_val=True):\n",
        "  \"\"\"\n",
        "  Loads Training and Test datasets\n",
        "  \"\"\"\n",
        "  image_size = (config['image_size'], config['image_size']) # (224, 224) originally\n",
        "  batch_size = config['batch_size']\n",
        "\n",
        "  if use_val:\n",
        "    train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        dataset_path + '/train',\n",
        "        image_size=image_size,\n",
        "        batch_size=batch_size,\n",
        "        validation_split = 0.2,\n",
        "        subset='both',\n",
        "        shuffle=True,\n",
        "        seed=1337\n",
        "    )\n",
        "  else:\n",
        "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        dataset_path + '/train',\n",
        "        image_size=image_size,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        seed=1337\n",
        "    )\n",
        "    val_ds = None\n",
        "\n",
        "  test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "      dataset_path + '/test',\n",
        "      image_size=image_size,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False\n",
        "  )\n",
        "  return train_ds, val_ds, test_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8BWsRK8OWLZ"
      },
      "source": [
        "## Data visualization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_class_counts(dataset):\n",
        "  n_muf=0\n",
        "  n_chi = 0\n",
        "  for images, labels in tqdm(dataset):\n",
        "    for label in labels:\n",
        "      if label == 0:\n",
        "        n_chi += 1\n",
        "      else:\n",
        "        n_muf += 1\n",
        "\n",
        "  print(n_muf, n_chi)"
      ],
      "metadata": {
        "id": "Owbg5GE-cBL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OwqYu0XOWLZ"
      },
      "outputs": [],
      "source": [
        "def show_sample_figures(images, labels):\n",
        "  plt.figure(figsize=(10, 10))\n",
        "\n",
        "  for cell in range(9):\n",
        "      i = random.randint(0, len(labels)-1)\n",
        "      ax = plt.subplot(3, 3, cell + 1)\n",
        "      plt.imshow(images[i]/255)\n",
        "      plt.title(int(labels[i]))\n",
        "      plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGPFSx1wOWLZ"
      },
      "source": [
        "## Data augmentation and optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b53EppWOWLa"
      },
      "outputs": [],
      "source": [
        "def augment_dataset(dataset, show_preview=False):\n",
        "  data_augmentation = keras.Sequential(\n",
        "      [\n",
        "          layers.RandomFlip(\"horizontal\"),\n",
        "          layers.RandomRotation(0.1),\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  # Show augmentation preview\n",
        "  if show_preview:\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for images, _ in dataset.take(1):\n",
        "        for i in range(9):\n",
        "            augmented_images = data_augmentation(images)\n",
        "            ax = plt.subplot(3, 3, i + 1)\n",
        "            plt.imshow(augmented_images[1].numpy().astype(\"uint8\"))\n",
        "            plt.axis(\"off\")\n",
        "\n",
        "  # Apply `data_augmentation` to the training images.\n",
        "  dataset = dataset.map(\n",
        "    lambda img, label: (data_augmentation(img), label),\n",
        "    num_parallel_calls=tf.data.AUTOTUNE,\n",
        "  )\n",
        "\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prefetch_datasets(*args):\n",
        "  args_list = [arg for arg in args]\n",
        "  # Prefetching samples in GPU memory helps maximize GPU utilization.\n",
        "  for i in range(len(args_list)):\n",
        "    args_list[i] = args_list[i].prefetch(tf.data.AUTOTUNE)\n",
        "  return args"
      ],
      "metadata": {
        "id": "0UDRrdhB9CEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QmlepllOWLb"
      },
      "source": [
        "# Model builders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSJDCoF9OWLb"
      },
      "outputs": [],
      "source": [
        "'''def make_model(config, input_shape, num_classes):\n",
        "  inputs = keras.Input(shape=input_shape)\n",
        "\n",
        "  # Entry block\n",
        "  x = layers.Rescaling(1.0 / 255)(inputs)\n",
        "  x = layers.Conv2D(64, 3, strides=2, padding=\"same\")(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "  previous_block_activation = x  # Set aside residual\n",
        "\n",
        "  for size in [128]:\n",
        "      x = layers.Activation(\"relu\")(x)\n",
        "      x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
        "      x = layers.BatchNormalization()(x)\n",
        "\n",
        "      x = layers.Activation(\"relu\")(x)\n",
        "      x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
        "      x = layers.BatchNormalization()(x)\n",
        "\n",
        "      x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "      # Project residual\n",
        "      residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n",
        "          previous_block_activation\n",
        "      )\n",
        "      x = layers.add([x, residual])  # Add back residual\n",
        "      previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "  x = layers.SeparableConv2D(256, 3, padding=\"same\")(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "  x = layers.GlobalAveragePooling2D()(x)\n",
        "  if num_classes == 2:\n",
        "      activation = \"sigmoid\"\n",
        "      units = 1\n",
        "  else:\n",
        "      activation = \"softmax\"\n",
        "      units = num_classes\n",
        "\n",
        "  x = layers.Dropout(config['dropout'])(x)\n",
        "  outputs = layers.Dense(units, activation=activation)(x)\n",
        "\n",
        "  return keras.Model(inputs, outputs)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BJ78JrXuHpu"
      },
      "outputs": [],
      "source": [
        "# Sources: https://stackoverflow.com/questions/46836358/keras-rgb-to-grayscale\n",
        "#          https://www.johndcook.com/blog/2009/08/24/algorithms-convert-color-grayscale/\n",
        "def greyscale(x):\n",
        "  # x has shape (batch, width, height, channels)\n",
        "  return (0.21 * x[:,:,:,:1]) + (0.72 * x[:,:,:,1:2]) + (0.07 * x[:,:,:,-1:])\n",
        "\n",
        "def make_model(config, input_shape, num_classes):\n",
        "  inputs = keras.Input(shape=input_shape)\n",
        "\n",
        "  x = layers.Lambda(greyscale)(inputs)\n",
        "  x = layers.Rescaling(1.0 / 255)(x)\n",
        "  x = layers.Flatten(input_shape=input_shape)(x)\n",
        "  x = layers.Dense(config['image_size'], activation='relu')(x)\n",
        "  x = layers.Dense(config['image_size'] * 2, activation='relu')(x)\n",
        "  x = layers.Dense(config['image_size'] * 2, activation='relu')(x)\n",
        "  x = layers.Dropout(config['dropout'])(x)\n",
        "\n",
        "  if num_classes == 2:\n",
        "    units = 1\n",
        "  else:\n",
        "    units = num_classes\n",
        "\n",
        "  outputs = layers.Dense(units, activation=\"sigmoid\")(x)\n",
        "\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  model.summary()\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and tuning"
      ],
      "metadata": {
        "id": "1M83TSSE-Dj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(config):\n",
        "  train, val, _ = load_datasets(config)\n",
        "  #print_class_counts(train)\n",
        "  #show_sample_figures(train)\n",
        "  train = augment_dataset(train, show_preview=False)\n",
        "  train, val = prefetch_datasets(train, val)\n",
        "\n",
        "  model = make_model(config, input_shape=(config.image_size, config.image_size) + (3,), num_classes=2)\n",
        "  keras.utils.plot_model(model, show_shapes=True)\n",
        "\n",
        "  epochs = config.epochs\n",
        "\n",
        "  model.compile(\n",
        "    optimizer=keras.optimizers.Adam(config.init_learning_rate),\n",
        "    loss=config.loss_fn,\n",
        "    metrics=[\"accuracy\"],\n",
        "  )\n",
        "\n",
        "  callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, start_from_epoch=0), WandbMetricsLogger(log_freq=2)]\n",
        "\n",
        "  history = model.fit(\n",
        "    train,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks,\n",
        "    validation_data=val,\n",
        "  )\n",
        "\n",
        "  return max(history.history['val_accuracy'])"
      ],
      "metadata": {
        "id": "TKsdWF-PIU7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_5fold(config):\n",
        "  train, _, _ = load_datasets(config, use_val=False)\n",
        "  train = augment_dataset(train, show_preview=False)\n",
        "\n",
        "  # Retrieve data and labels from training set\n",
        "  train_unb = train.unbatch()\n",
        "  train_x = []\n",
        "  train_y = []\n",
        "  for images, labels in train_unb.map(lambda x, y: (x, y)):\n",
        "    train_x.append(images)\n",
        "    train_y.append(labels)\n",
        "  train_x = np.array(train_x)\n",
        "  train_y = np.array(train_y)\n",
        "\n",
        "  accuracies = []\n",
        "\n",
        "  for train_indexes, val_indexes in StratifiedKFold(n_splits=5).split(train_x, train_y):\n",
        "\n",
        "    fold_train_x = []\n",
        "    fold_train_y = []\n",
        "    for i in train_indexes:\n",
        "      fold_train_x.append(train_x[i])\n",
        "      fold_train_y.append(train_y[i])\n",
        "\n",
        "    fold_val_x = []\n",
        "    fold_val_y = []\n",
        "    for i in val_indexes:\n",
        "      fold_val_x.append(train_x[i])\n",
        "      fold_val_y.append(train_y[i])\n",
        "\n",
        "    model = make_model(config, input_shape=(config.image_size, config.image_size) + (3,), num_classes=2)\n",
        "\n",
        "    epochs = config.epochs\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(config.init_learning_rate),\n",
        "        loss=config.loss_fn,\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "\n",
        "    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, start_from_epoch=15), WandbMetricsLogger(log_freq=2)]\n",
        "\n",
        "    history = model.fit(\n",
        "        np.array(fold_train_x), np.array(fold_train_y),\n",
        "        epochs=epochs,\n",
        "        batch_size=config.batch_size,\n",
        "        callbacks=callbacks,\n",
        "        validation_data=(np.array(fold_val_x), np.array(fold_val_y)),\n",
        "    )\n",
        "    accuracies.append(max(history.history['val_accuracy']))\n",
        "\n",
        "    '''\n",
        "    # training preds\n",
        "    for i in range(10):\n",
        "      img = fold_train_x[i]\n",
        "      predictions = model.predict(np.reshape(img,(1,64,64,3)))\n",
        "      score = float(predictions[0])\n",
        "      ground_truth = fold_train_y[i]\n",
        "      print(f\"This image is {100 * (1 - score):.2f}% chihuahua and {100 * score:.2f}% muffin. and it should be {ground_truth}\")\n",
        "      plt.imshow(img/255)\n",
        "      plt.title(int(ground_truth))\n",
        "      plt.axis(\"off\")\n",
        "      plt.show()\n",
        "\n",
        "    # validation preds\n",
        "    for i in range(10):\n",
        "      img = fold_val_x[i]\n",
        "      predictions = model.predict(np.reshape(img,(1,64,64,3)))\n",
        "      score = float(predictions[0])\n",
        "      ground_truth = fold_val_y[i]\n",
        "      print(f\"This image is {100 * (1 - score):.2f}% chihuahua and {100 * score:.2f}% muffin. and it should be {ground_truth}\")\n",
        "      plt.imshow(img/255)\n",
        "      plt.title(int(ground_truth))\n",
        "      plt.axis(\"off\")\n",
        "      plt.show()\n",
        "    '''\n",
        "\n",
        "  return np.mean(accuracies)"
      ],
      "metadata": {
        "id": "ixBxpjDFp3xN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1: Define training function to run on the sweep\n",
        "def main():\n",
        "    wandb.init()\n",
        "    val_accuracy = objective(wandb.config)\n",
        "    wandb.log({'val_accuracy': val_accuracy})\n",
        "\n",
        "# 2: Define the search space\n",
        "sweep_configuration = {\n",
        "    'method': 'bayes',\n",
        "    'metric':\n",
        "    {\n",
        "        'goal': 'maximize',\n",
        "        'name': 'val_accuracy'\n",
        "    },\n",
        "    'parameters':\n",
        "    {\n",
        "        'init_learning_rate': {\n",
        "            'distribution': 'log_uniform_values', 'max': 1e-2, 'min': 1e-5\n",
        "        },\n",
        "        'batch_size': {\n",
        "          # integers between 32 and 1024\n",
        "          # with evenly-distributed logarithms\n",
        "          'distribution': 'q_log_uniform_values',\n",
        "          'q': 32,\n",
        "          'min': 32,\n",
        "          'max': 1024,\n",
        "        },\n",
        "        'dropout': {\n",
        "          'values': [0.2, 0.3, 0.4, 0.5, 0.7]\n",
        "        },\n",
        "    }\n",
        "}\n",
        "sweep_configuration['parameters'].update({attribute:{'value':value} for attribute, value in configs.items()})\n",
        "\n",
        "# 3: Start the sweep\n",
        "sweep_id = wandb.sweep(\n",
        "    sweep=sweep_configuration,\n",
        "    project='MuffinChihuahuas'\n",
        ")\n",
        "\n",
        "wandb.agent(sweep_id, function=main, count=10)"
      ],
      "metadata": {
        "id": "VB30QRYvAFgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train on best values\n"
      ],
      "metadata": {
        "id": "IW50tUsz8AeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api = wandb.Api()\n",
        "sweep = api.sweep(f\"minigi/MuffinChihuahuas/sweeps/8mvl2ozv\")\n",
        "\n",
        "# Get best run parameters\n",
        "best_run = sweep.best_run(order='val_accuracy')\n",
        "best_parameters = best_run.config\n",
        "print(best_parameters)"
      ],
      "metadata": {
        "id": "xNilE8De8AJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_run.summary"
      ],
      "metadata": {
        "id": "9X7t6xb48bae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! wandb sweep --stop minigi/MuffinChihuahuas/8mvl2ozv"
      ],
      "metadata": {
        "id": "woJ0PLZc_Kzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project='MuffinChihuahua', config=best_parameters)\n",
        "print(wandb.config)\n",
        "final_value = objective_5fold(wandb.config)\n",
        "print(final_value)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "Zes6YcyxMYN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bclMguTeOWLc"
      },
      "source": [
        "# Run inference on new data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGch4GVyOWLc"
      },
      "outputs": [],
      "source": [
        "\"\"\"def keralize_img(path):\n",
        "  img = keras.utils.load_img(\n",
        "    path, target_size=configs.image_size\n",
        "  )\n",
        "  img_array = keras.utils.img_to_array(img)\n",
        "  img_array = tf.expand_dims(img_array, 0)  # Create batch axis\n",
        "  return img_array\n",
        "\n",
        "chihuahua_img = keralize_img(dataset_path + '/test/muffin/img_0_67.jpg')\n",
        "muffin_img = keralize_img(dataset_path + '/test/chihuahua/img_0_1107.jpg')\n",
        "\n",
        "def predict(img):\n",
        "  model.load_weights(\"/content/save_at_7.keras\")\n",
        "  predictions = model.predict(img)\n",
        "  score = float(predictions[0])\n",
        "  print(f\"This image is {100 * (1 - score):.2f}% chihuahua and {100 * score:.2f}% muffin.\")\n",
        "\n",
        "predict(chihuahua_img)\n",
        "predict(muffin_img)\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz4HiZuwIJBt"
      },
      "source": [
        "# Debug utilities"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''def test_cv(configs):\n",
        "    wandb.init(project='MuffinChihuahuas', config=configs)\n",
        "    val_accuracy = objective_5fold(wandb.config)\n",
        "    val_accuracy'''"
      ],
      "metadata": {
        "id": "0hSw6SkVEGCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''def test_stratified_5fold(dataset):\n",
        "  train_x, train_y = retrieve_data_and_labels(dataset)\n",
        "  for train_indexes, val_indexes in StratifiedKFold(n_splits=5).split(train_x, train_y):\n",
        "    print(\"######\")\n",
        "    print(\"training\", train_x[train_indexes].shape, train_y[train_indexes].shape)\n",
        "    print(np.bincount(train_y[train_indexes]))\n",
        "    print(\"validation\", train_x[val_indexes].shape, train_y[val_indexes].shape)\n",
        "    print(np.bincount(train_y[val_indexes]))'''"
      ],
      "metadata": {
        "id": "ow2PHVst6hEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test_cv(wandb.config)"
      ],
      "metadata": {
        "id": "aanZsvaef9fY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''dataset, _, _ = load_datasets(wandb.config, use_val=False)\n",
        "dataset = augment_dataset(dataset, show_preview=False)\n",
        "test_stratified_5fold(dataset)'''"
      ],
      "metadata": {
        "id": "rLO8V6zxgEK6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}